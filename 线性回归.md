### <center>线性回归</center>
#### [线性规划][1]：
&emsp;还是按照简介的思路来说，以简单的**一元线性回归**（一元代表只有一个未知自变量）做介绍。
&emsp;&emsp;有n组数据，自变量x(x1,x2,…,xn)，因变量y(y1,y2,…,yn)，然后我们假设它们之间的关系是：f(x)=ax+b。那么线性回归的目标就是如何让f(x)和y之间的差异最小，换句话说就是a，b取什么值的时候f(x)和y最接近。
这里我们得先解决另一个问题，就是如何衡量f(x)和y之间的差异。在回归问题中，[均方误差](https://blog.csdn.net/imJaron/article/details/79127674)是回归任务中最常用的性能度量（自行百度一下均方误差）。记J(a,b)为f(x)和y之间的差异，即

![](https://img-blog.csdnimg.cn/20201031225640628.png#pic_center)

i代表n组数据中的第i组。
&emsp;&emsp;这里称J(a,b)为损失函数，明显可以看出它是个二次函数，即凸函数（这里的凸函数对应中文教材的凹函数），所以有最小值。当J(a,b)取最小值的时候，f(x)和y的差异最小，然后我们可以通过J(a,b)取最小值来确定a和b的值。

到这里可以说线性回归就这些了，只不过我们还需要解决其中最关键的问题：确定a和b的值。

下面介绍三种方法来确定a和b的值：
1.[最小二乘法][2]:
[介绍1][2]：简介：多元线性模型、最小二乘法与梯度下降法对比、限制与解决方法、最小二乘法拟合多项式非线性函数
&emsp;&emsp;既然损失函数J(a,b)是凸函数，那么分别关于a和b对J(a,b)求偏导，并令其为零解出a和b。这里直接给出结果：
![](https://img-blog.csdnimg.cn/4b62083c1c3741edbaf120578e69694e.png#pic_center)
![](https://img-blog.csdnimg.cn/10be7913a84849db8ded3c117ec88e3e.png#pic_center)
解得：
![](https://img-blog.csdnimg.cn/20201031231002725.png#pic_center)
![](https://img-blog.csdnimg.cn/20201031230931899.png#pic_center)
2.[梯度下降法][3]:
[介绍1][3]
[介绍2][4]:类型：(批量梯度下降算法、随机梯度下降算法、小批量梯度下降算法)
&emsp;&emsp;首先你得先了解一下梯度的概念：梯度的本意是一个向量（矢量），表示某一函数（该函数一般是二元及以上的）在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。
&emsp;&emsp;当函数是一元函数时，梯度就是导数。这里我们用一个最简单的例子来讲解梯度下降法，然后推广理解更为复杂的函数。
还是用上面的例子，有n组数据，自变量x(x1,x2,…,xn)，因变量y(y1,y2,…,yn)，但这次我们假设它们之间的关系是：f(x)=ax。记J(a)为f(x)和y之间的差异，即
![](https://img-blog.csdnimg.cn/20201101095525256.png#pic_center)
&emsp;&emsp;在梯度下降法中，需要我们先给参数a赋一个预设值，然后再一点一点的修改a，直到J(a)取最小值时，确定a的值。下面直接给出梯度下降法的公式（其中α为正数）：
![](https://img-blog.csdnimg.cn/20201101100653809.png#pic_center)
&emsp;&emsp;下面解释一下公式的意义，J(a)和a的关系如下图，
![](https://img-blog.csdnimg.cn/20201101111133605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY5NzE5OA==,size_16,color_FFFFFF,t_70#pic_center)
&emsp;&emsp;假设给a取的预设值是a1的话，那么a对J(a)的导数为负数，则

<div align=center>
<img src="https://img-blog.csdnimg.cn/20201101102930327.png#pic_center"/>
</div>

&emsp;&emsp;也为负数，所以
<div align=center>
<img src="https://img-blog.csdnimg.cn/20201101103258227.png#pic_center"/>
</div>
&emsp;&emsp;意味着a向右移一点。然后重复这个动作，直到J(a)到达最小值。
同理，假设给a取的预设值是a2的话，那么a对J(a)的导数为正数，则
<div align=center>
<img src="https://img-blog.csdnimg.cn/20201101103258227.png#pic_center"/>
</div>
&emsp;&emsp;意味着a向左移一点。然后重复这个动作，直到J(a)到达最小值。
所以我们可以看到，不管a的预设值取多少，J(a)经过梯度下降法的多次重复后，最后总能到达最小值。

&emsp;&emsp;这里再举个生活中的栗子，**梯度下降法中随机给a赋一个预设值就好比你随机出现在一个山坡上，然后这时候你想以最快的方式走到山谷的最低点，那么你就得判断你的下一步该往那边走，走完一步之后同样再次判断下一步的方向，以此类推就能走到山谷的最低点了。而公式中的α我们称它为学习率，在栗子中可以理解为你每一步跨出去的步伐有多大，α越大，步伐就越大。（实际中α的取值不能太大也不能太小，太大会造成损失函数J接近最小值时，下一步就越过去了。好比在你接近山谷的最低点时，你步伐太大一步跨过去了，下一步往回走的时候又是如此跨过去，永远到达不了最低点；α太小又会造成移动速度太慢，因为我们当然希望在能确保走到最低点的前提下越快越好。）**

&emsp;&emsp;到这里，梯度下降法的思想你基本就理解了，只不过在栗子中我们是用最简单的情况来说明，而事实上梯度下降法可以推广到多元线性函数上，这里直接给出公式，理解上（需要你对多元函数的相关知识有了解）和上面的栗子殊途同归。
&emsp;&emsp;假设有n组数据，其中目标值（因变量）与特征值（自变量）之间的关系为：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20201101105348538.png#pic_center"/>
</div>

其中i表示第i组数据，损失函数为：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20201101105534166.png#pic_center"/>
</div>
梯度下降法：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20201101105748613.png#pic_center"/>
</div>

梯度下降算法代码实现：

```python
from numpy import *

# 数据集大小 即20个数据点
m = 20
# x的坐标以及对应的矩阵
X0 = ones((m, 1))  # 生成一个m行1列的向量，也就是x0，全是1
X1 = arange(1, m+1).reshape(m, 1)  # 生成一个m行1列的向量，也就是x1，从1到m
X = hstack((X0, X1))  # 按照列堆叠形成数组，其实就是样本数据
# 对应的y坐标
Y = array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m, 1)
# 学习率
alpha = 0.01


# 定义代价函数
def cost_function(theta, X, Y):
    diff = dot(X, theta) - Y  # dot() 数组需要像矩阵那样相乘，就需要用到dot()
    return (1/(2*m)) * dot(diff.transpose(), diff)


# 定义代价函数对应的梯度函数
def gradient_function(theta, X, Y):
    diff = dot(X, theta) - Y
    return (1/m) * dot(X.transpose(), diff)


# 梯度下降迭代
def gradient_descent(X, Y, alpha):
    theta = array([1, 1]).reshape(2, 1)
    gradient = gradient_function(theta, X, Y)
    while not all(abs(gradient) <= 1e-5):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, Y)
    return theta


optimal = gradient_descent(X, Y, alpha)
print('optimal:', optimal)
print('cost function:', cost_function(optimal, X, Y)[0][0])


# 根据数据画出对应的图像
def plot(X, Y, theta):
    import matplotlib.pyplot as plt
    ax = plt.subplot(111)  # 这是我改的
    ax.scatter(X, Y, s=30, c="red", marker="s")
    plt.xlabel("X")
    plt.ylabel("Y")
    x = arange(0, 21, 0.2)  # x的范围
    y = theta[0] + theta[1]*x
    ax.plot(x, y)
    plt.show()


plot(X1, Y, optimal)
```

3.[正规方程][5]：
[介绍1][5]：简介：公式推导、正规方程与梯度下降法比较

[1]:https://blog.csdn.net/weixin_44697198/article/details/109405212 "机器学习算法——线性回归"
[2]:https://blog.csdn.net/xiewenrui1996/article/details/107418803 "机器学习十大经典算法之最小二乘法"
[3]:https://blog.csdn.net/qq_41800366/article/details/86583789 "梯度下降法1"
[4]:https://blog.csdn.net/JaysonWong/article/details/119818497 "梯度下降法2"
[5]:https://blog.csdn.net/sd9110110/article/details/53558821 "正规方程1"